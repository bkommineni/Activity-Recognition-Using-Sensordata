{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy\n",
      "  Downloading scipy-1.0.0-cp27-cp27m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (16.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 16.8MB 58kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python2.7/site-packages (from scipy)\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.0.0\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         User    Bin1,x    Bin2,x    Bin3,x    Bin4,x    Bin5,x    Bin6,x  \\\n",
      "0    Bhargavi  0.014085  0.018109  0.022133  0.062374  0.197183  0.283702   \n",
      "1    Bhargavi  0.003597  0.012590  0.023381  0.048561  0.097122  0.224820   \n",
      "2    Bhargavi  0.018349  0.029358  0.082569  0.155963  0.234862  0.220183   \n",
      "3    Bhargavi  0.003810  0.009524  0.001905  0.009524  0.045714  0.211429   \n",
      "4    Bhargavi  0.010989  0.027473  0.084249  0.104396  0.216117  0.236264   \n",
      "5    Bhargavi  0.014572  0.034608  0.054645  0.123862  0.227687  0.262295   \n",
      "6    Bhargavi  0.014953  0.056075  0.091589  0.166355  0.218692  0.218692   \n",
      "7    Bhargavi  0.005629  0.013133  0.058161  0.176360  0.298311  0.213884   \n",
      "8    Bhargavi  0.009690  0.025194  0.050388  0.077519  0.151163  0.193798   \n",
      "9    Bhargavi  0.009276  0.020408  0.044527  0.092764  0.150278  0.218924   \n",
      "10   Bhargavi  0.009276  0.020408  0.044527  0.092764  0.150278  0.218924   \n",
      "11   Bhargavi  0.009107  0.010929  0.029144  0.061931  0.098361  0.209472   \n",
      "12   Bhargavi  0.014493  0.027174  0.052536  0.079710  0.115942  0.166667   \n",
      "13   Bhargavi  0.007634  0.019084  0.026718  0.066794  0.095420  0.146947   \n",
      "14   Bhargavi  0.014170  0.040486  0.082996  0.129555  0.155870  0.176113   \n",
      "15   Bhargavi  0.013462  0.023077  0.076923  0.115385  0.190385  0.173077   \n",
      "16     Surada  0.007092  0.056738  0.148936  0.258865  0.262411  0.147163   \n",
      "17     Surada  0.005245  0.006993  0.019231  0.075175  0.174825  0.283217   \n",
      "18     Surada  0.001678  0.006711  0.020134  0.122483  0.226510  0.280201   \n",
      "19     Surada  0.006897  0.020690  0.082759  0.241379  0.327586  0.186207   \n",
      "20     Surada  0.006723  0.042017  0.082353  0.225210  0.265546  0.193277   \n",
      "21     Surada  0.010239  0.013652  0.027304  0.143345  0.300341  0.237201   \n",
      "22     Surada  0.003247  0.003247  0.003247  0.019481  0.107143  0.274351   \n",
      "23     Surada  0.005658  0.007072  0.018388  0.076379  0.188119  0.274399   \n",
      "24     Surada  0.028286  0.063228  0.056572  0.076539  0.161398  0.149750   \n",
      "25     Surada  0.007924  0.026941  0.047544  0.128368  0.163233  0.172742   \n",
      "26     Surada  0.001572  0.012579  0.058176  0.064465  0.172956  0.160377   \n",
      "27     Surada  0.017771  0.030695  0.053312  0.067851  0.158320  0.124394   \n",
      "28     Surada  0.033748  0.053286  0.104796  0.177620  0.122558  0.138544   \n",
      "29     Surada  0.014035  0.043860  0.094737  0.114035  0.142105  0.107018   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "387    Surada  0.012195  0.041812  0.090592  0.121951  0.121951  0.175958   \n",
      "388    Surada  0.008993  0.039568  0.098921  0.097122  0.151079  0.214029   \n",
      "389    Surada  0.014733  0.031308  0.088398  0.162063  0.121547  0.189687   \n",
      "390    Surada  0.028725  0.086176  0.095153  0.116697  0.141831  0.190305   \n",
      "391    Surada  0.009242  0.038817  0.075786  0.133087  0.170055  0.164510   \n",
      "392    Surada  0.010889  0.039927  0.090744  0.197822  0.217786  0.181488   \n",
      "393    Surada  0.010850  0.010850  0.092224  0.186257  0.236890  0.229656   \n",
      "394    Surada  0.001818  0.000000  0.012727  0.078182  0.243636  0.338182   \n",
      "395    Surada  0.008977  0.021544  0.046679  0.168761  0.254937  0.236984   \n",
      "396    Surada  0.014467  0.047016  0.103074  0.162749  0.238698  0.233273   \n",
      "397    Surada  0.027125  0.077758  0.133816  0.179024  0.231465  0.188065   \n",
      "398    Surada  0.005747  0.026820  0.065134  0.155172  0.197318  0.210728   \n",
      "399    Surada  0.010830  0.010830  0.045126  0.079422  0.133574  0.189531   \n",
      "400    Surada  0.006981  0.040140  0.080279  0.178010  0.247818  0.235602   \n",
      "401    Surada  0.007105  0.007105  0.042629  0.065719  0.140320  0.218472   \n",
      "402    Surada  0.003584  0.023297  0.059140  0.177419  0.320789  0.275986   \n",
      "403    Surada  0.003604  0.012613  0.032432  0.113514  0.183784  0.273874   \n",
      "404    Surada  0.003623  0.007246  0.039855  0.125000  0.211957  0.295290   \n",
      "405    Surada  0.005455  0.007273  0.032727  0.110909  0.149091  0.229091   \n",
      "406    Surada  0.010889  0.034483  0.079855  0.172414  0.281307  0.266788   \n",
      "407    Surada  0.028986  0.067029  0.101449  0.153986  0.233696  0.166667   \n",
      "408    Surada  0.009107  0.043716  0.087432  0.125683  0.187614  0.234973   \n",
      "409    Surada  0.001818  0.009091  0.061818  0.072727  0.172727  0.194545   \n",
      "410    Surada  0.010870  0.016304  0.034420  0.092391  0.152174  0.181159   \n",
      "411    Surada  0.005455  0.020000  0.072727  0.198182  0.307273  0.185455   \n",
      "412    Surada  0.001650  0.004950  0.011551  0.062706  0.132013  0.240924   \n",
      "413    Surada  0.009091  0.038182  0.081818  0.170909  0.250909  0.214545   \n",
      "414    Surada  0.003515  0.010545  0.042179  0.089631  0.163445  0.249561   \n",
      "415    Surada  0.003306  0.006612  0.023140  0.069421  0.181818  0.254545   \n",
      "416    Surada  0.001733  0.000000  0.001733  0.015598  0.039861  0.175043   \n",
      "\n",
      "       Bin7,x    Bin8,x    Bin9,x   ...     AvgAbsDiff-y  AvgAbsDiff-z  \\\n",
      "0    0.167002  0.112676  0.056338   ...         0.706879      1.368683   \n",
      "1    0.258993  0.181655  0.098921   ...         0.607448      1.393734   \n",
      "2    0.113761  0.075229  0.047706   ...         0.618102      1.488376   \n",
      "3    0.339048  0.257143  0.091429   ...         0.538423      1.280605   \n",
      "4    0.177656  0.084249  0.043956   ...         0.564663      1.231150   \n",
      "5    0.129326  0.100182  0.034608   ...         0.596857      1.339832   \n",
      "6    0.119626  0.044860  0.037383   ...         0.666639      1.451335   \n",
      "7    0.116323  0.088180  0.013133   ...         0.623408      1.283183   \n",
      "8    0.153101  0.168605  0.118217   ...         1.353052      2.182665   \n",
      "9    0.168831  0.128015  0.103896   ...         1.398889      2.325614   \n",
      "10   0.168831  0.128015  0.103896   ...         1.398889      2.325614   \n",
      "11   0.191257  0.162113  0.118397   ...         1.523726      2.734043   \n",
      "12   0.206522  0.150362  0.137681   ...         1.541130      2.355774   \n",
      "13   0.183206  0.188931  0.162214   ...         1.384787      2.620416   \n",
      "14   0.174089  0.113360  0.072874   ...         1.577212      2.608892   \n",
      "15   0.157692  0.130769  0.084615   ...         1.543906      3.117911   \n",
      "16   0.065603  0.030142  0.007092   ...         0.569099      1.201950   \n",
      "17   0.281469  0.094406  0.031469   ...         0.558938      1.342595   \n",
      "18   0.189597  0.100671  0.033557   ...         0.548371      1.419010   \n",
      "19   0.089655  0.032759  0.005172   ...         0.556859      1.363672   \n",
      "20   0.099160  0.048739  0.028571   ...         0.560262      1.461892   \n",
      "21   0.155290  0.058020  0.030717   ...         0.535824      1.509142   \n",
      "22   0.293831  0.183442  0.087662   ...         0.624265      1.767758   \n",
      "23   0.244696  0.117397  0.038190   ...         0.618266      1.510236   \n",
      "24   0.148087  0.133111  0.103161   ...         1.702646      3.533312   \n",
      "25   0.141046  0.171157  0.110935   ...         1.684454      3.821432   \n",
      "26   0.191824  0.182390  0.113208   ...         1.782417      3.847146   \n",
      "27   0.153473  0.155089  0.135703   ...         1.970185      3.599470   \n",
      "28   0.161634  0.092362  0.090586   ...         2.173589      3.489777   \n",
      "29   0.091228  0.152632  0.138596   ...         2.209412      4.356226   \n",
      "..        ...       ...       ...   ...              ...           ...   \n",
      "387  0.217770  0.137631  0.050523   ...         2.238746      3.323948   \n",
      "388  0.197842  0.122302  0.041367   ...         2.246595      3.492299   \n",
      "389  0.197053  0.141805  0.038674   ...         2.422839      3.548327   \n",
      "390  0.206463  0.096948  0.026930   ...         2.303309      3.431369   \n",
      "391  0.264325  0.096118  0.029575   ...         2.241707      3.420657   \n",
      "392  0.136116  0.059891  0.045372   ...         0.706636      2.014988   \n",
      "393  0.122966  0.070524  0.025316   ...         0.664910      1.791999   \n",
      "394  0.205455  0.083636  0.025455   ...         0.710748      1.832472   \n",
      "395  0.150808  0.062837  0.026930   ...         0.701828      1.697946   \n",
      "396  0.122966  0.048825  0.016275   ...         0.654733      1.705669   \n",
      "397  0.097649  0.036166  0.018083   ...         0.711052      1.771503   \n",
      "398  0.187739  0.105364  0.030651   ...         0.671820      1.729146   \n",
      "399  0.176895  0.164260  0.095668   ...         0.661366      1.640137   \n",
      "400  0.155323  0.041885  0.010471   ...         0.684551      1.642184   \n",
      "401  0.198934  0.156306  0.097691   ...         0.683737      1.548178   \n",
      "402  0.100358  0.026882  0.008961   ...         0.637827      1.296858   \n",
      "403  0.225225  0.109910  0.030631   ...         0.568035      1.270649   \n",
      "404  0.210145  0.070652  0.028986   ...         0.682328      1.402953   \n",
      "405  0.205455  0.123636  0.076364   ...         0.701051      1.315521   \n",
      "406  0.107078  0.038113  0.003630   ...         0.679390      1.358899   \n",
      "407  0.141304  0.072464  0.023551   ...         0.696637      1.355491   \n",
      "408  0.165756  0.080146  0.041894   ...         0.612659      1.345917   \n",
      "409  0.207273  0.138182  0.070909   ...         0.600900      1.296443   \n",
      "410  0.208333  0.139493  0.103261   ...         0.679369      1.255221   \n",
      "411  0.138182  0.043636  0.020000   ...         0.610408      1.234915   \n",
      "412  0.224422  0.198020  0.084158   ...         0.613415      1.155267   \n",
      "413  0.138182  0.054545  0.023636   ...         0.635045      1.170293   \n",
      "414  0.214411  0.135325  0.061511   ...         0.546315      1.085644   \n",
      "415  0.209917  0.132231  0.084298   ...         0.682193      1.201847   \n",
      "416  0.322357  0.275563  0.098787   ...         0.575689      1.224615   \n",
      "\n",
      "     AvgAcc-x  AvgAcc-y   AvgAcc-z  StdDev-x  StdDev-y  StdDev-z  AvgResAcc  \\\n",
      "0    0.172889  1.309198   9.354637  0.446622  0.855555  1.560657   0.472090   \n",
      "1   -0.094222  0.971846   9.412232  0.438568  0.722373  1.579282   0.528508   \n",
      "2    0.009069  1.153406   9.402436  0.440231  0.754276  1.682175   0.518711   \n",
      "3   -0.049911  1.177769   9.418344  0.438225  0.645937  1.445476   0.500173   \n",
      "4   -0.050326  1.218036   9.434501  0.367049  0.681928  1.377831   0.521303   \n",
      "5   -0.135971  1.337527   9.428817  0.413898  0.726492  1.522971   0.525033   \n",
      "6   -0.048942  1.523129   9.388868  0.500561  0.833047  1.645922   0.511868   \n",
      "7    0.163861  1.782161   9.354661  0.382216  0.742376  1.450116   0.509792   \n",
      "8   -0.173441  3.311797   8.846435  1.716998  1.646268  2.593726   0.501903   \n",
      "9   -0.498847  3.298127   8.944683  1.884955  1.663476  2.736768   0.530271   \n",
      "10  -0.498847  3.298127   8.944683  1.884955  1.663476  2.736768   0.530271   \n",
      "11  -0.325388  2.925025   9.038051  2.102797  1.832673  3.169020   0.543524   \n",
      "12  -0.734042  2.616102   9.169893  2.027755  1.862300  2.904351   0.548169   \n",
      "13  -0.094398  2.193208   9.311489  1.982231  1.693523  3.132830   0.519307   \n",
      "14  -0.506650  2.525788   9.159650  2.102343  1.916348  3.209730   0.490830   \n",
      "15  -0.262366  2.964098   9.025664  2.258316  1.915014  3.642544   0.518347   \n",
      "16   0.011319  4.425237   9.111398  0.593937  0.704404  1.452545   0.573956   \n",
      "17  -0.034742  3.857096   9.373967  0.617774  0.700981  1.647402   0.582805   \n",
      "18  -0.026301  3.558441   9.505669  0.625548  0.683931  1.701889   0.608097   \n",
      "19  -0.226683  4.101810   9.307511  0.635878  0.689762  1.596005   0.592785   \n",
      "20  -0.157995  3.364304   9.585559  0.539673  0.697192  1.709819   0.607183   \n",
      "21  -0.147489  3.083381   9.702210  0.618913  0.674593  1.824053   0.599608   \n",
      "22  -0.152309  3.256841   9.643844  0.688357  0.792468  2.103219   0.631184   \n",
      "23  -0.118578  3.344286   9.582768  0.608719  0.787371  1.782347   0.721711   \n",
      "24  -0.197039  2.495260   9.952164  2.519256  2.056709  4.312242   0.653516   \n",
      "25  -0.157411  2.527302   9.913438  2.282053  2.097683  4.628678   0.680132   \n",
      "26   0.016950  2.700436   9.865781  2.194890  2.151511  4.619545   0.683469   \n",
      "27  -0.227913  2.920581   9.792938  2.230875  2.363501  4.443121   0.667127   \n",
      "28  -0.092700  2.990111   9.812758  2.600424  2.580470  4.369378   0.618035   \n",
      "29  -0.443717  2.153217  10.023958  2.886388  2.669843  5.323684   0.637360   \n",
      "..        ...       ...        ...       ...       ...       ...        ...   \n",
      "387 -0.039073  1.894270   9.963397  2.083765  2.728629  4.129036   0.619436   \n",
      "388 -0.303648  2.009326   9.970204  2.059988  2.706104  4.340114   0.601628   \n",
      "389 -0.340918  2.186582   9.905978  2.328097  2.986394  4.445848   0.593752   \n",
      "390 -0.409559  2.027125   9.938377  2.267340  2.797991  4.226556   0.604136   \n",
      "391 -0.184752  1.459003  10.086110  2.410289  2.721550  4.222693   0.589485   \n",
      "392 -0.445742  2.346833   9.904254  0.729014  0.867680  2.299945   0.565650   \n",
      "393 -0.603895  2.576893   9.836976  0.557877  0.862484  2.071525   0.566997   \n",
      "394 -0.623356  2.572211   9.821528  0.665671  0.887759  2.120875   0.563459   \n",
      "395 -0.465196  2.678939   9.805479  0.517155  0.881660  1.941645   0.570181   \n",
      "396 -0.582616  2.950865   9.724397  0.534530  0.803639  1.949544   0.566225   \n",
      "397 -0.633107  2.444824   9.856184  0.600278  0.887928  2.022562   0.566365   \n",
      "398 -0.370814  2.487894   9.895812  0.521718  0.849886  1.951783   0.536084   \n",
      "399 -0.433373  2.862330   9.772487  0.566917  0.816847  1.893683   0.567794   \n",
      "400 -0.414000  2.983904   9.723091  0.534605  0.837724  1.876079   0.586531   \n",
      "401 -0.569215  3.260750   9.617341  0.462293  0.833836  1.771708   0.575597   \n",
      "402 -0.344652  2.751124   9.818596  0.562738  0.829929  1.538966   0.572758   \n",
      "403 -0.109191  2.971524   9.703862  0.499087  0.712597  1.489098   0.565968   \n",
      "404 -0.303927  3.066914   9.682002  0.460835  0.852929  1.659759   0.564251   \n",
      "405 -0.261727  2.996006   9.693895  0.461490  0.861915  1.558452   0.561423   \n",
      "406 -0.279066  3.002242   9.712229  0.466346  0.849103  1.612784   0.563615   \n",
      "407 -0.192549  3.040172   9.685921  0.503940  0.862505  1.625311   0.564081   \n",
      "408 -0.055724  2.838563   9.760799  0.494046  0.789444  1.597131   0.561045   \n",
      "409 -0.252395  2.747791   9.801528  0.475796  0.745960  1.527160   0.562719   \n",
      "410 -0.097923  2.861656   9.775318  0.446617  0.835856  1.509974   0.565404   \n",
      "411 -0.264111  3.068913   9.709657  0.386682  0.762420  1.452050   0.562783   \n",
      "412 -0.192081  2.969971   9.703252  0.466679  0.753206  1.397570   0.618006   \n",
      "413 -0.463920  2.923755   9.714199  0.432004  0.803724  1.396875   0.561564   \n",
      "414 -0.386964  3.155469   9.651730  0.395484  0.690552  1.303276   0.580675   \n",
      "415 -0.230854  2.749393   9.773625  0.503655  0.849599  1.469253   0.618262   \n",
      "416 -0.448217  3.197396   9.611194  0.591266  0.734592  1.532271   0.588503   \n",
      "\n",
      "       Label  \n",
      "0    walking  \n",
      "1    walking  \n",
      "2    walking  \n",
      "3    walking  \n",
      "4    walking  \n",
      "5    walking  \n",
      "6    walking  \n",
      "7    walking  \n",
      "8    running  \n",
      "9    running  \n",
      "10   running  \n",
      "11   running  \n",
      "12   running  \n",
      "13   running  \n",
      "14   running  \n",
      "15   running  \n",
      "16   walking  \n",
      "17   walking  \n",
      "18   walking  \n",
      "19   walking  \n",
      "20   walking  \n",
      "21   walking  \n",
      "22   walking  \n",
      "23   walking  \n",
      "24   running  \n",
      "25   running  \n",
      "26   running  \n",
      "27   running  \n",
      "28   running  \n",
      "29   running  \n",
      "..       ...  \n",
      "387  running  \n",
      "388  running  \n",
      "389  running  \n",
      "390  running  \n",
      "391  running  \n",
      "392  walking  \n",
      "393  walking  \n",
      "394  walking  \n",
      "395  walking  \n",
      "396  walking  \n",
      "397  walking  \n",
      "398  walking  \n",
      "399  walking  \n",
      "400  walking  \n",
      "401  walking  \n",
      "402  walking  \n",
      "403  walking  \n",
      "404  walking  \n",
      "405  walking  \n",
      "406  walking  \n",
      "407  walking  \n",
      "408  walking  \n",
      "409  walking  \n",
      "410  walking  \n",
      "411  walking  \n",
      "412  walking  \n",
      "413  walking  \n",
      "414  walking  \n",
      "415  walking  \n",
      "416  walking  \n",
      "\n",
      "[417 rows x 45 columns]\n",
      "['User', 'Bin1,x', 'Bin2,x', 'Bin3,x', 'Bin4,x', 'Bin5,x', 'Bin6,x', 'Bin7,x', 'Bin8,x', 'Bin9,x', 'Bin10,x', 'Bin1,y', 'Bin2,y', 'Bin3,y', 'Bin4,y', 'Bin5,y', 'Bin6,y', 'Bin7,y', 'Bin8,y', 'Bin9,y', 'Bin10,y', 'Bin1,z', 'Bin2,z', 'Bin3,z', 'Bin4,z', 'Bin5,z', 'Bin6,z', 'Bin7,z', 'Bin8,z', 'Bin9,z', 'Bin10,z', 'TimeDiffPeaks-x', 'TimeDiffPeaks-y', 'TimeDiffPeaks-z', 'AvgAbsDiff-x', 'AvgAbsDiff-y', 'AvgAbsDiff-z', 'AvgAcc-x', 'AvgAcc-y', 'AvgAcc-z', 'StdDev-x', 'StdDev-y', 'StdDev-z', 'AvgResAcc', 'Label']\n"
     ]
    }
   ],
   "source": [
    "perceptron = pd.read_csv('../FeaturesCsvFile/featuresfile.csv')\n",
    "header = list(perceptron.head(1))\n",
    "print perceptron\n",
    "print header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.014084507042253521 0.01810865191146881 0.02213279678068411 ...,\n",
      "  0.8555552081764867 1.5606569938953019 0.4720899709524997]\n",
      " [0.0035971223021582736 0.012589928057553957 0.02338129496402877 ...,\n",
      "  0.7223733614901274 1.5792815854925566 0.5285080446119012]\n",
      " [0.01834862385321101 0.029357798165137613 0.08256880733944953 ...,\n",
      "  0.7542756929430607 1.6821749125497956 0.5187113000651693]\n",
      " ..., \n",
      " [0.0035149384885764497 0.010544815465729346 0.0421792618629174 ...,\n",
      "  0.6905519776091801 1.3032763729675667 0.5806752753410925]\n",
      " [0.0033057851239669434 0.006611570247933887 0.0231404958677686 ...,\n",
      "  0.8495993319578534 1.469252793708261 0.6182617061257438]\n",
      " [0.0017331022530329288 0.0 0.0017331022530329288 ..., 0.7345915371492492\n",
      "  1.5322706889895572 0.5885031716413449]]\n",
      "['walking' 'walking' 'walking' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'running' 'running' 'running' 'running' 'running' 'running'\n",
      " 'running' 'running' 'walking' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'running' 'running' 'running' 'running'\n",
      " 'running' 'running' 'running' 'running' 'walking' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'walking' 'running' 'running' 'running'\n",
      " 'running' 'running' 'running' 'running' 'running' 'running' 'running'\n",
      " 'running' 'running' 'running' 'running' 'running' 'running' 'running'\n",
      " 'running' 'running' 'running' 'running' 'running' 'running' 'running'\n",
      " 'running' 'running' 'running' 'running' 'running' 'running' 'running'\n",
      " 'running' 'running' 'running' 'running' 'running' 'running' 'running'\n",
      " 'running' 'running' 'running' 'running' 'running' 'running' 'running'\n",
      " 'running' 'running' 'running' 'running' 'running' 'running' 'running'\n",
      " 'running' 'running' 'running' 'running' 'running' 'running' 'running'\n",
      " 'running' 'running' 'running' 'running' 'running' 'running' 'running'\n",
      " 'running' 'running' 'running' 'running' 'running' 'running' 'running'\n",
      " 'running' 'running' 'running' 'running' 'running' 'running' 'running'\n",
      " 'running' 'running' 'running' 'running' 'walking' 'walking' 'running'\n",
      " 'running' 'running' 'running' 'running' 'running' 'running' 'running'\n",
      " 'running' 'running' 'running' 'running' 'running' 'running' 'running'\n",
      " 'running' 'walking' 'walking' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'running' 'running' 'running' 'running' 'running' 'running'\n",
      " 'running' 'running' 'running' 'running' 'running' 'running' 'running'\n",
      " 'running' 'running' 'running' 'running' 'running' 'running' 'running'\n",
      " 'running' 'running' 'walking' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'walking' 'walking' 'walking' 'walking'\n",
      " 'running' 'running' 'running' 'running' 'running' 'running' 'running'\n",
      " 'running' 'running' 'running' 'running' 'running' 'running' 'running'\n",
      " 'running' 'running' 'running' 'running' 'running' 'running' 'running'\n",
      " 'running' 'running' 'running' 'running' 'running' 'running' 'running'\n",
      " 'running' 'running' 'running' 'running' 'running' 'running' 'running'\n",
      " 'running' 'running' 'running' 'running' 'running' 'running' 'running'\n",
      " 'walking' 'walking' 'walking' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'walking']\n",
      "417\n",
      "417\n"
     ]
    }
   ],
   "source": [
    "X = perceptron.values[:, 1:44]\n",
    "y = perceptron.values[:, 44]\n",
    "print X\n",
    "print y\n",
    "print len(X)\n",
    "print len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron(alpha=0.0001, class_weight=None, eta0=0.1, fit_intercept=True,\n",
      "      max_iter=40, n_iter=None, n_jobs=1, penalty=None, random_state=1,\n",
      "      shuffle=True, tol=None, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "ppn = Perceptron(max_iter=40, eta0=0.1, random_state=1)\n",
    "print ppn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, eta0=0.1, fit_intercept=True,\n",
       "      max_iter=40, n_iter=None, n_jobs=1, penalty=None, random_state=1,\n",
       "      shuffle=True, tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ppn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['walking' 'walking' 'walking' 'walking' 'running' 'walking' 'running'\n",
      " 'running' 'walking' 'walking' 'walking' 'running' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'walking' 'running' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'walking' 'running' 'running' 'walking'\n",
      " 'walking' 'walking' 'walking' 'running' 'walking' 'running' 'walking'\n",
      " 'running' 'walking' 'running' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'running' 'walking' 'walking' 'running'\n",
      " 'running' 'walking' 'running' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'running' 'running' 'walking' 'running' 'walking' 'walking'\n",
      " 'running' 'running' 'running' 'walking' 'walking' 'walking' 'walking'\n",
      " 'walking' 'walking' 'walking' 'walking' 'walking' 'running' 'running'\n",
      " 'running' 'running' 'walking' 'running' 'walking' 'walking' 'running'\n",
      " 'walking' 'running' 'walking' 'running' 'running' 'running' 'walking'\n",
      " 'walking' 'walking' 'running' 'walking' 'running' 'walking' 'walking'\n",
      " 'walking' 'walking' 'running' 'running' 'running' 'walking' 'walking'\n",
      " 'walking' 'running' 'running' 'walking' 'running' 'walking' 'running'\n",
      " 'running' 'walking' 'walking' 'running' 'running' 'walking' 'walking'\n",
      " 'walking' 'walking' 'running' 'walking' 'walking' 'running' 'running']\n"
     ]
    }
   ],
   "source": [
    "print y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Accuracy Score : 0.99\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Accuracy Score : %.2f' % accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Perceptron Score: 0.99\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Perceptron Score: %.2f' % ppn.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confustion Metrix\n",
      "[[53  1]\n",
      " [ 0 72]]\n"
     ]
    }
   ],
   "source": [
    "print ('Confustion Metrix')\n",
    "print (confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(estimator=ppn, X=X_train, y=y_train, cv=32, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV accuracy scores: [ 1.          1.          0.9         1.          1.          1.          1.\n",
      "  1.          1.          0.77777778  1.          1.          0.88888889\n",
      "  0.77777778  1.          1.          0.88888889  1.          1.          1.\n",
      "  0.88888889  0.88888889  0.77777778  1.          1.          0.55555556\n",
      "  0.44444444  1.          0.77777778  0.88888889  1.          1.        ]\n"
     ]
    }
   ],
   "source": [
    "print ('\\nCV accuracy scores: %s' % scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV accuracy: 0.920 +/- 0.134\n"
     ]
    }
   ],
   "source": [
    "print ('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
